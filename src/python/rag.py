
import sys, os, time
import logging
import configparser
#from sklearn.feature_extraction.text import TfidfVectorizer
#from sklearn.metrics.pairwise import cosine_similarity
from rag_core import (
    prepare_model,
    prepare_knowledge,
    retrieve,
    build_prompt,
    build_prompt_for_translation,
    generate_answer,
    TOP_K,
    MAX_LENGTH
)
from rag_utils import (
    print_elapsed_time_and_update,
    read_file,
    write_text_to_file
)


logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

DATA_PATH = "data.txt"


MODEL_ID = "OpenVINO/Mistral-7B-Instruct-v0.3-int4-ov"
MODEL_PATH = "Mistral-7B-Instruct-v0.3-int4-ov"
DEVICE = "GPU"


"""
def chunk_text(s, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    s = s.strip()
    if not s:
        return []
    chunks = []
    start = 0
    n = len(s)
    while start < n:
        end = min(start + size, n)
        chunk = s[start:end]
        chunks.append(chunk)
        if end == n:
            break
        start = end - overlap
        if start < 0:
            start = 0
    return chunks

# ===== ì¸ë±ìŠ¤ êµ¬ì¶• (ë¬¸ì n-gramìœ¼ë¡œ í•œê¸€ ê²€ìƒ‰ í’ˆì§ˆ ê°œì„ ) =====
def build_vector_store(chunks):
    # analyzer='char'ë¡œ 3~5gram: í•œê¸€/ì˜ë¬¸ í˜¼ìš© í…ìŠ¤íŠ¸ì— ê°•í•¨
    vectorizer = TfidfVectorizer(analyzer="char", ngram_range=(3, 5))
    X = vectorizer.fit_transform(chunks)
    return vectorizer, X

# ===== ê²€ìƒ‰ =====
def retrieve(query, vectorizer, X, chunks, top_k=TOP_K):
    qv = vectorizer.transform([query])
    sims = cosine_similarity(qv, X)[0]
    idxs = sims.argsort()[::-1][:top_k]
    results = [(i, float(sims[i]), chunks[i]) for i in idxs]
    print("ğŸ” ê²€ìƒ‰ ê²°ê³¼:", results)
    return results


# ===== í”„ë¡¬í”„íŠ¸ êµ¬ì„± =====
def build_messages(user_question, retrieved):
    # ì§€ì‹ ì»¨í…ìŠ¤íŠ¸ë¥¼ S1, S2...ë¡œ ë¶™ì„
    src_lines = []
    for rank, (idx, score, text) in enumerate(retrieved, start=1):
        # ë„ˆë¬´ ê¸´ ì¡°ê°ì€ ì‚´ì§ ì ‘ê¸°
        clipped = textwrap.shorten(text.replace("\n", " "), width=800, placeholder=" â€¦")
        src_lines.append(f"[S{rank}] {clipped}")

    context_block = "\n".join(src_lines) if src_lines else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"
    user_prompt = (
        f"ì§€ì‹ ì»¨í…ìŠ¤íŠ¸:\n{context_block}\n\n"
        f"ì§ˆë¬¸: {user_question}\n\n"
        "ê·œì¹™:\n"
        "0) ê° ë‹µë³€ì˜ ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ '<ë>'ì´ë¼ê³  ì ì–´ ì¢…ë£Œí•˜ì„¸ìš”.\n"
        "1) ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©.\n"
        "2) ì—†ìœ¼ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.<ë>'ë¼ê³  ë‹µë³€.\n"
        "4) 3ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°íˆ.\n"
    )

    return user_prompt
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt}
    ]

    return messages
"""


def load_model(model_path: str):
    global MODEL_ID, MODEL_PATH, DEVICE

    model_path = os.path.join(model_path, MODEL_PATH)
    logger.info(f"model_path: {model_path}")
    pipe = prepare_model(model_id=MODEL_ID, model_path=model_path, device=DEVICE, offline=True)
    print_elapsed_time_and_update("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    return pipe


def unload_model(pipe):
    del pipe
    import gc
    gc.collect()
    print_elapsed_time_and_update("ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")


def set_file_path(file_path):
    global DATA_PATH

    contents = read_file(file_path)
    write_text_to_file(DATA_PATH, contents)
    chunks, vectorizer, X = prepare_knowledge(DATA_PATH)

    return chunks, vectorizer, X

def one_time_rag(input_str, vectorizer, X, chunks, pipe, max_length=MAX_LENGTH, top_k=TOP_K) -> str:
    retrieved = retrieve(input_str, vectorizer, X, chunks, top_k)
    logger.debug("ğŸ“„ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸:")
    prompt = build_prompt(input_str, retrieved)
    logger.debug(prompt)
    logger.debug("ğŸ¤– AI ì‘ë‹µ ìƒì„± ì¤‘...")
    outputs = generate_answer(pipe, prompt, max_length=max_length)
    return outputs

def translate(input_str, pipe, max_length=MAX_LENGTH, top_k=TOP_K) -> str:
    prompt = build_prompt_for_translation(input_str)
    logger.debug(prompt)
    logger.debug("ğŸ¤– AI ì‘ë‹µ ìƒì„± ì¤‘...")
    outputs = generate_answer(pipe, prompt, max_length=max_length)
    return outputs


def interactive_rag(
    data_path=DATA_PATH,
    model_id=MODEL_ID,
    model_path=MODEL_PATH,
    device=DEVICE,
    max_length=MAX_LENGTH,
    top_k=TOP_K
):
    pipe, chunks, vectorizer, X = load_model()
    logger.debug("RAG ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ê°™ì€ í´ë”ì˜ data.txtë¥¼ ì§€ì‹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    logger.debug("ì¢…ë£Œí•˜ë ¤ë©´ /exit ë˜ëŠ” /quit ì„ ì…ë ¥í•˜ì„¸ìš”.\n")
    while True:
        input_str = input("Enter your question (or type 'exit' to quit): ").strip()
        if not input_str:
            continue
        if input_str.lower() == 'exit':
            break
        if "max_length" in input_str:
            max_length = int(input_str.split("max_length")[-1].strip())
            logger.debug(f"Using max_length: {max_length}")
            continue
        begin_time = time.time()
    logger.debug("ğŸ” ì§€ì‹ ê²€ìƒ‰ ì¤‘...")
    retrieved = retrieve(input_str, vectorizer, X, chunks, top_k)
    logger.debug("ğŸ“„ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸:")
    prompt = build_prompt(input_str, retrieved)
    logger.debug(prompt)
    logger.debug("ğŸ¤– AI ì‘ë‹µ ìƒì„± ì¤‘...")
    outputs = generate_answer(pipe, prompt, max_length=max_length)
    logger.debug(outputs)

if __name__ == "__main__":
    #interactive_rag()
    load_model()
