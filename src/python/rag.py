
import huggingface_hub as hf_hub
import openvino_genai as ov_genai
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import torch, sys, os, textwrap, time
import time
import sys
import os

DATA_PATH = "data.txt"
CHUNK_SIZE = 900          # ë¬¸ì ê¸°ì¤€ ì¡°ê° í¬ê¸°
CHUNK_OVERLAP = 150       # ê²¹ì¹¨
TOP_K = 5                 # ìƒìœ„ ëª‡ ê°œ ì¡°ê°ì„ ë„£ì„ì§€


model_id = "OpenVINO/Mistral-7B-Instruct-v0.3-int4-ov"
model_path = "Mistral-7B-Instruct-v0.3-int4-ov"

device = "GPU"
max_length = 200

begin_time = 0

def print_elapsed_time_and_update(tag: str):
    global begin_time
    assert begin_time > 0, "begin_timeì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    print(f"Time for {tag}: {time.time() - begin_time:.2f} seconds")
    begin_time = time.time()

# ===== ìœ í‹¸: íŒŒì¼ ë¡œë“œ & ì²­í¬ =====
def read_text(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"ì§€ì‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def chunk_text(s, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    s = s.strip()
    if not s:
        return []
    chunks = []
    start = 0
    n = len(s)
    while start < n:
        end = min(start + size, n)
        chunk = s[start:end]
        chunks.append(chunk)
        if end == n:
            break
        start = end - overlap
        if start < 0:
            start = 0
    return chunks

# ===== ì¸ë±ìŠ¤ êµ¬ì¶• (ë¬¸ì n-gramìœ¼ë¡œ í•œê¸€ ê²€ìƒ‰ í’ˆì§ˆ ê°œì„ ) =====
def build_vector_store(chunks):
    # analyzer='char'ë¡œ 3~5gram: í•œê¸€/ì˜ë¬¸ í˜¼ìš© í…ìŠ¤íŠ¸ì— ê°•í•¨
    vectorizer = TfidfVectorizer(analyzer="char", ngram_range=(3, 5))
    X = vectorizer.fit_transform(chunks)
    return vectorizer, X

# ===== ê²€ìƒ‰ =====
def retrieve(query, vectorizer, X, chunks, top_k=TOP_K):
    qv = vectorizer.transform([query])
    sims = cosine_similarity(qv, X)[0]
    idxs = sims.argsort()[::-1][:top_k]
    results = [(i, float(sims[i]), chunks[i]) for i in idxs]
    print("ğŸ” ê²€ìƒ‰ ê²°ê³¼:", results)
    return results


# ===== í”„ë¡¬í”„íŠ¸ êµ¬ì„± =====
def build_messages(user_question, retrieved):
    # ì§€ì‹ ì»¨í…ìŠ¤íŠ¸ë¥¼ S1, S2...ë¡œ ë¶™ì„
    src_lines = []
    for rank, (idx, score, text) in enumerate(retrieved, start=1):
        # ë„ˆë¬´ ê¸´ ì¡°ê°ì€ ì‚´ì§ ì ‘ê¸°
        clipped = textwrap.shorten(text.replace("\n", " "), width=800, placeholder=" â€¦")
        src_lines.append(f"[S{rank}] {clipped}")

    context_block = "\n".join(src_lines) if src_lines else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"
    user_prompt = (
        f"ì§€ì‹ ì»¨í…ìŠ¤íŠ¸:\n{context_block}\n\n"
        f"ì§ˆë¬¸: {user_question}\n\n"
        "ê·œì¹™:\n"
        "0) ê° ë‹µë³€ì˜ ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ '<ë>'ì´ë¼ê³  ì ì–´ ì¢…ë£Œí•˜ì„¸ìš”.\n"
        "1) ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©.\n"
        "2) ì—†ìœ¼ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.<ë>'ë¼ê³  ë‹µë³€.\n"
        "4) 3ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°íˆ.\n"
    )

    return user_prompt
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": user_prompt}
    ]

    return messages


def main():
    global begin_time
    global model_id, model_path, device, max_length, begin_time
    begin_time = time.time()

    hf_hub.snapshot_download(model_id, local_dir=model_path)

    pipe = ov_genai.LLMPipeline(model_path, device)

    print("RAG ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ê°™ì€ í´ë”ì˜ data.txtë¥¼ ì§€ì‹ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ /exit ë˜ëŠ” /quit ì„ ì…ë ¥í•˜ì„¸ìš”.\n")

    # ì§€ì‹ ë¡œë“œ & ì¸ë±ìŠ¤
    try:
        corpus = read_text(DATA_PATH)
    except Exception as e:
        print(f"ì§€ì‹ ë¡œë“œ ì˜¤ë¥˜: {e}", file=sys.stderr)
        sys.exit(1)

    chunks = chunk_text(corpus, CHUNK_SIZE, CHUNK_OVERLAP)
    if not chunks:
        print("data.txtê°€ ë¹„ì–´ ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.", file=sys.stderr)
        exit(1)

    vectorizer, X = build_vector_store(chunks)


    while True:
        input_str = input("Enter your question (or type 'exit' to quit): ")
        input_str = input_str.strip()
        if not input_str or input_str == '':
            continue
        if input_str.lower() == 'exit':
            break

        if "max_length" in input_str:
            max_length = int(input_str.split("max_length")[-1].strip())
            print(f"Using max_length: {max_length}")
            continue

        begin_time = time.time()

        # ê²€ìƒ‰
        print("ğŸ” ì§€ì‹ ê²€ìƒ‰ ì¤‘...")
        retrieved = retrieve(input_str, vectorizer, X, chunks, TOP_K)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        print("ğŸ“„ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸:")
        message = build_messages(input_str, retrieved)
        print(message)

        print("ğŸ¤– AI ì‘ë‹µ ìƒì„± ì¤‘...")
        outputs = pipe.generate(message, max_length=max_length)
        print_elapsed_time_and_update("generation")
        print(outputs)

if __name__ == "__main__":
    main()
