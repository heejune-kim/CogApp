import huggingface_hub as hf_hub
import openvino_genai as ov_genai
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import textwrap
import os
import time
import glob
from rag_utils import (
    get_logger,
    is_debug_mode,
)

CHUNK_SIZE = 900
CHUNK_OVERLAP = 150
TOP_K = 5
MAX_LENGTH = 200

# ëª¨ë¸ ì¤€ë¹„

RAG_PROMPT = (
    "ì§€ì‹ ì»¨í…ìŠ¤íŠ¸:\n[CONTEXT_BLOCK]\n\n"
    "ì§ˆë¬¸: [USER_QUESTION]\n\n"
    "ê·œì¹™:\n"
    "0) ê° ë‹µë³€ì˜ ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ '<ë>'ì´ë¼ê³  ì ì–´ ì¢…ë£Œí•˜ì„¸ìš”.\n"
    "1) ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©.\n"
    "2) ì—†ìœ¼ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.<ë>'ë¼ê³  ë‹µë³€.\n"
    "3) 3ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°íˆ.\n"
)

TRANSLATION_PROMPT = (
    "ë‹¤ìŒ ë¬¸ì¥ì„ ì˜ì–´ë¡œ ë²ˆì—­í•´ ì£¼ì„¸ìš”:\"[USER_QUESTION]\"\n\n"
    "ê·œì¹™:\n"
    "0) ê° ë‹µë³€ì˜ ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ '<ë>'ì´ë¼ê³  ì ì–´ ì¢…ë£Œí•˜ì„¸ìš”.\n"
    "1) ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í•  ê²ƒ.\n"
    "2) ë¬¸í™”ì  ë§¥ë½ì„ ê³ ë ¤í•  ê²ƒ.\n"
    "3) ë¬¸ë²•ì ìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ì‘ì„±í•  ê²ƒ.\n"
    "4) ëŒ€ë‹µì— í•œê¸€ì€ ì ˆëŒ€ë¡œ ë“¤ì–´ê°€ë©´ ì•ˆë¨.\n"
)

REQUIRED_PATTERNS = ["*.xml", "*.bin", "*.json", "tokenizer.*", "*.txt", "*.model", "*.vocab", "*.merges", "*.sentencepiece*"]

RAG_PROMPT_PATH = None
TRANSLATION_PROMPT_PATH = None


def read_rag_prompt_from_file(prompt_path: str = None) -> str:
    global RAG_PROMPT

    logger = get_logger()
    if prompt_path is None:
        prompt_path = RAG_PROMPT_PATH
    if prompt_path and os.path.isfile(prompt_path):
        with open(prompt_path, "r", encoding="utf-8") as pf:
            prompt = pf.read()

            logger.debug(f"RAG prompt read from file: {prompt_path}")
            logger.debug(f"RAG prompt content: {prompt[:100]}...")  # ì²˜ìŒ 100ìë§Œ ë¡œê·¸ì— ì¶œë ¥

            RAG_PROMPT = prompt
        return prompt
    return None


def read_translation_prompt_from_file(prompt_path: str = None) -> str:
    global TRANSLATION_PROMPT

    if prompt_path is None:
        prompt_path = TRANSLATION_PROMPT_PATH

    if prompt_path and os.path.isfile(prompt_path):
        with open(prompt_path, "r", encoding="utf-8") as pf:
            prompt = pf.read()

            logger = get_logger()
            logger.debug(f"Translation prompt read from file: {prompt_path}")
            logger.debug(f"Translation prompt content: {prompt[:100]}...")  # ì²˜ìŒ 100ìë§Œ ë¡œê·¸ì— ì¶œë ¥

            TRANSLATION_PROMPT = prompt
        return prompt
    return None


def set_prompt_path(rag_prompt_path: str = None, translation_prompt_path: str = None):
    global RAG_PROMPT_PATH, TRANSLATION_PROMPT_PATH
    logger = get_logger()

    if rag_prompt_path and os.path.isfile(rag_prompt_path):
        RAG_PROMPT_PATH = rag_prompt_path
        logger.debug(f"RAG prompt path set to: {RAG_PROMPT_PATH}")

    if translation_prompt_path and os.path.isfile(translation_prompt_path):
        TRANSLATION_PROMPT_PATH = translation_prompt_path
        logger.debug(f"Translation prompt path set to: {TRANSLATION_PROMPT_PATH}")


def set_prompts(rag_prompt: str = None, translation_prompt: str = None):
    global RAG_PROMPT, TRANSLATION_PROMPT

    if rag_prompt:
        RAG_PROMPT = rag_prompt
    if translation_prompt:
        TRANSLATION_PROMPT = translation_prompt


def set_rag_params(chunk_size: int, chunk_overlap: int, top_k: int, max_length: int):
    global CHUNK_SIZE, CHUNK_OVERLAP, TOP_K, MAX_LENGTH

    CHUNK_SIZE = chunk_size
    CHUNK_OVERLAP = chunk_overlap
    TOP_K = top_k
    MAX_LENGTH = max_length


CHUNK_SIZE = 900
CHUNK_OVERLAP = 150
TOP_K = 5
MAX_LENGTH = 200
def _has_openvino_snapshot(local_dir: str) -> bool:
    """IR(xml/bin)ì™€ í† í¬ë‚˜ì´ì € íŒŒì¼ì´ ìˆëŠ”ì§€ ê°„ë‹¨íˆ ì ê²€."""
    if not os.path.isdir(local_dir):
        return False
    has_xml = len(glob.glob(os.path.join(local_dir, "*.xml"))) > 0
    has_bin = len(glob.glob(os.path.join(local_dir, "*.bin"))) > 0
    # í† í¬ë‚˜ì´ì € ê´€ë ¨ íŒŒì¼ì€ ëª¨ë¸ë§ˆë‹¤ ë‹¬ë¼ì„œ ëŠìŠ¨í•˜ê²Œ ì²´í¬
    has_tok = any(glob.glob(os.path.join(local_dir, pat)) for pat in ["tokenizer.json", "tokenizer.model", "vocab.json", "vocab.txt"])
    return has_xml and has_bin and has_tok

def _download_if_needed(
    repo_id: str,
    local_dir: str,
    *,
    cache_dir: str | None = None,
    revision: str | None = None,
    token: str | None = None,
    allow_patterns: list[str] | None = None,
    force: bool = False,
    offline: bool = False,
) -> str:
    """
    í•„ìš”í•œ íŒŒì¼ì´ ì—†ìœ¼ë©´ snapshot_downloadë¡œ ë‚´ë ¤ë°›ê³ ,
    ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ê²½ë¡œë§Œ ë°˜í™˜.
    """
    if allow_patterns is None:
        allow_patterns = REQUIRED_PATTERNS

    # ì´ë¯¸ ì™„ë¹„ë˜ì–´ ìˆê³  ê°•ì œ ì¬ë‹¤ìš´ë¡œë“œê°€ ì•„ë‹ˆë©´ í†µê³¼
    if not force and _has_openvino_snapshot(local_dir):
        return local_dir

    # ì˜¤í”„ë¼ì¸ ê°•ì œ ì‹œ, ì—†ìœ¼ë©´ ì—ëŸ¬
    if offline:
        # ì˜¤í”„ë¼ì¸ì´ë©´ ë¡œì»¬ì— ìˆëŠ” ê²ƒë§Œ í—ˆìš©
        if _has_openvino_snapshot(local_dir):
            return local_dir
        raise FileNotFoundError(f"offline=True ì´ê³ , '{local_dir}'ì— í•„ìš”í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ì˜¨ë¼ì¸ ëª¨ë“œ: í•„ìš”í•œ ê²ƒë§Œ ë°›ì•„ì„œ local_dirì— 'ì‹¤íŒŒì¼ ë³µì‚¬' (symlink X)
    return hf_hub.snapshot_download(
        repo_id=repo_id,
        revision=revision,
        repo_type="model",
        cache_dir=cache_dir,
        local_dir=local_dir,
        local_dir_use_symlinks=False,   # ìœˆë„ìš°/ì´ë™ì‹ ê²½ë¡œ ì•ˆì „
        allow_patterns=allow_patterns,
        token=token,
        resume_download=True,
        force_download=force,
        local_files_only=False
    )

def prepare_model(
    model_id: str,
    model_path: str,
    device: str,
    *,
    cache_dir: str | None = None,
    revision: str | None = None,
    token: str | None = None,
    force_download: bool = False,
    offline: bool = False
):
    """
    - model_pathì— ëª¨ë¸ì´ ì™„ë¹„ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - ì—†ìœ¼ë©´ í•„ìš”í•œ íŒŒì¼ë“¤ë§Œ ë°›ì•„ì„œ ë°°ì¹˜
    - offline=Trueë©´ ë„¤íŠ¸ì›Œí¬ ì—†ì´ ë¡œì»¬ë§Œ ì‚¬ìš©
    """
    logger = get_logger()
    logger.debug(f"prepare_model called with model_id={model_id}, model_path={model_path}, device={device}, offline={offline}")

    t0 = time.time()
    resolved_dir = _download_if_needed(
        repo_id=model_id,
        local_dir=model_path,
        cache_dir=cache_dir,
        revision=revision,
        token=token,
        force=force_download,
        offline=offline
    )
    logger.debug(f"Model files are ready at: {resolved_dir}")

    # ë²„ì „ ì²´í¬ ê²½ê³ ë¥¼ ë¬´ì‹œí•˜ê³  ëª¨ë¸ ë¡œë“œ ì‹œë„
    import warnings
    import io
    import sys

    # stderrë¥¼ ìº¡ì²˜í•˜ì—¬ ë²„ì „ ê²½ê³  ë©”ì‹œì§€ ì–µì œ
    original_stderr = sys.stderr
    try:
        # ê²½ê³  í•„í„° ì„¤ì •
        warnings.filterwarnings('ignore', category=RuntimeWarning)

        # stderrë¥¼ ì„ì‹œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ (ë²„ì „ ë¶ˆì¼ì¹˜ ë©”ì‹œì§€ëŠ” C++ ë ˆë²¨ì—ì„œ stderrë¡œ ì¶œë ¥ë¨)
        captured_stderr = io.StringIO()
        sys.stderr = captured_stderr

        pipe = ov_genai.LLMPipeline(resolved_dir, device)

    except RuntimeError as e:
        # ë²„ì „ ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ì¸ ê²½ìš° ë¬´ì‹œí•˜ê³  ì¬ì‹œë„
        error_msg = str(e)
        if "binary compatible" in error_msg.lower() or "version" in error_msg.lower():
            logger.warning(f"Version mismatch warning ignored: {error_msg}")
            logger.info("Attempting to load model despite version mismatch...")
            # ê°•ì œë¡œ ë‹¤ì‹œ ì‹œë„ (ì´ë¯¸ DLLì´ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ì„±ê³µí•  ìˆ˜ ìˆìŒ)
            pipe = ov_genai.LLMPipeline(resolved_dir, device)
        else:
            raise
    finally:
        # stderr ë³µì›
        sys.stderr = original_stderr
        stderr_output = captured_stderr.getvalue()
        # ë²„ì „ ê´€ë ¨ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë¡œê·¸ ì¶œë ¥
        if stderr_output and "binary compatible" not in stderr_output.lower():
            logger.debug(f"Model loading messages: {stderr_output}")

    # ì„ íƒ: ë¡œë“œ ì‹œê°„ ë¡œê·¸
    logger.debug(f"[prepare_model] ready in {time.time() - t0:.2f}s at: {resolved_dir}")
    return pipe


"""
def prepare_model(model_id, model_path, device, cache_dir=None):
    hf_hub.snapshot_download(model_id, local_dir=model_path, cache_dir=cache_dir)
    pipe = ov_genai.LLMPipeline(model_path, device)
    return pipe
"""

# ë°ì´í„° ë¡œë“œ ë° ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„

def prepare_knowledge(data_path, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"ì§€ì‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
    with open(data_path, "r", encoding="utf-8", errors="ignore") as f:
        corpus = f.read()
    chunks = chunk_text(corpus, chunk_size, chunk_overlap)
    if not chunks:
        raise ValueError("ì§€ì‹ íŒŒì¼ì´ ë¹„ì–´ ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
    vectorizer, X = build_vector_store(chunks)
    return chunks, vectorizer, X

# í…ìŠ¤íŠ¸ ì²­í¬

def chunk_text(s, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    s = s.strip()
    if not s:
        return []
    chunks = []
    start = 0
    n = len(s)
    while start < n:
        end = min(start + size, n)
        chunk = s[start:end]
        chunks.append(chunk)
        if end == n:
            break
        start = end - overlap
        if start < 0:
            start = 0
    return chunks

# ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶•

def build_vector_store(chunks):
    vectorizer = TfidfVectorizer(analyzer="char", ngram_range=(3, 5))
    X = vectorizer.fit_transform(chunks)
    return vectorizer, X

# ê²€ìƒ‰

def retrieve(query, vectorizer, X, chunks, top_k=TOP_K):
    qv = vectorizer.transform([query])
    sims = cosine_similarity(qv, X)[0]
    idxs = sims.argsort()[::-1][:top_k]
    results = [(i, float(sims[i]), chunks[i]) for i in idxs]
    return results

# í”„ë¡¬í”„íŠ¸ ìƒì„±

def build_prompt(user_question, retrieved, prompt_template=RAG_PROMPT):
    src_lines = []
    for rank, (idx, score, text) in enumerate(retrieved, start=1):
        clipped = textwrap.shorten(text.replace("\n", " "), width=800, placeholder=" â€¦")
        src_lines.append(f"[S{rank}] {clipped}")
    context_block = "\n".join(src_lines) if src_lines else "(ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ)"
    if is_debug_mode():
        prompt_template = read_rag_prompt_from_file()
        print("ğŸ” ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¸”ë¡:\n", context_block)
        print("ğŸ” ì‚¬ìš©ì ì§ˆë¬¸:\n", user_question)
        print("ğŸ” RAG í”„ë¡¬í”„íŠ¸:\n", RAG_PROMPT)
    user_prompt = prompt_template.replace("[CONTEXT_BLOCK]", context_block).replace("[USER_QUESTION]", user_question)
    return user_prompt


def build_prompt_for_translation(user_question, prompt_template=TRANSLATION_PROMPT):
    if is_debug_mode():
        prompt_template = read_translation_prompt_from_file()
        print("ğŸ” ì‚¬ìš©ì ì§ˆë¬¸(ë²ˆì—­ìš©):\n", user_question)
        print("ğŸ” ë²ˆì—­ í”„ë¡¬í”„íŠ¸:\n", prompt_template)
    user_prompt = prompt_template.replace("[USER_QUESTION]", user_question)
    return user_prompt

# ë‹µë³€ ìƒì„±

def generate_answer(pipe, prompt, max_length=MAX_LENGTH):
    answer = pipe.generate(prompt, max_length=max_length)
    if "<ë>" in answer:
        answer = answer.split("<ë>")[0].strip()
    if "(ë)" in answer:
        answer = answer.split("(ë)")[0].strip()
    return answer

if __name__ == "__main__":
    pass  # í…ŒìŠ¤íŠ¸ìš© ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ëŠ” ì—†ìŒ
